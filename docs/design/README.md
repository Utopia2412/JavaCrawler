# 设计文档

## 系统架构

本爬虫项目采用简单而高效的单体架构，主要包含以下组件：

### 核心组件

1. 爬虫引擎
   - URL管理器：管理待爬取的URL队列和已访问URL集合
   - 下载器：负责网页内容的下载和请求管理
   - 解析器：解析HTML内容，提取所需信息
   - 数据存储：将爬取的内容保存到文件系统

2. 反爬虫策略
   - 请求延迟：控制请求频率
   - User-Agent轮换：模拟不同浏览器
   - 请求头管理：添加必要的HTTP头信息

## 类设计

### TextCrawl（基础文本爬虫）
- 功能：单页面文本爬取
- 主要方法：
  - main()：程序入口
  - jsoup()：执行爬取逻辑

### AutoTextCrawler（自动文本爬虫）
- 功能：自动遍历网站并爬取文本内容
- 主要方法：
  - crawlAllChapters()：遍历并爬取所有章节
  - crawlChapterContent()：爬取单个章节内容
  - isValidUrl()：URL有效性验证

### AutoImaCrawler（自动图片爬虫）
- 功能：自动爬取网站图片
- 主要方法：
  - crawlAndDownload()：爬取和下载图片
  - processUrl()：处理单个URL
  - downloadImages()：下载图片
  - extractNewUrls()：提取新URL

## 数据流

1. URL处理流程
```
输入URL -> URL验证 -> URL队列 -> 下载内容 -> 解析内容 -> 提取新URL -> URL队列
```

2. 内容处理流程
```
下载内容 -> 内容解析 -> 数据清洗 -> 文件保存
```

## 性能优化

1. 内存管理
   - 使用队列控制URL处理
   - 及时释放不需要的资源
   - 限制最大处理URL数量

2. 请求优化
   - 实现请求重试机制
   - 添加请求延迟
   - 优化请求超时设置

## 扩展性设计

系统设计考虑了以下扩展点：

1. 新增爬虫类型
   - 继承现有爬虫类
   - 实现自定义解析逻辑

2. 配置项扩展
   - 支持自定义URL过滤规则
   - 可配置的请求参数
   - 可自定义输出格式

## 异常处理

1. 网络异常
   - 实现重试机制
   - 记录错误信息
   - 优雅降级

2. 解析异常
   - 空值处理
   - 格式验证
   - 异常日志记录

## 安全性考虑

1. 请求安全
   - 遵守robots.txt规则
   - 控制请求频率
   - 避免恶意URL

2. 文件安全
   - 路径合法性验证
   - 文件名净化
   - 权限控制